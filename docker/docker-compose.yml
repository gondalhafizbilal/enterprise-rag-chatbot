version: "3.8"

services:
  server:
    build: ../server
    container_name: chatbot_server
    ports:
      - "8000:8000"
    environment:
      - WEAVIATE_URL=http://weaviate:8080
      - OLLAMA_HOST=http://llama3:11434
      - OLLAMA_MODEL=gemma:2b
      - LOG_LEVEL=info
    depends_on:
      - weaviate
      - llama3
    command: >
      sh -c "sleep 5 &&
             python /app/app/init_schema.py &&
             uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"
    volumes:
      - ../server/app/storage:/app/app/storage
      - ../server/app:/app/app
    tty: true # Ensures logs stream correctly
    stdin_open: true
    logging:
      driver: "json-file" # Default but explicit
      options:
        max-size: "10m" # Rotate logs after 10MB
        max-file: "5" # Keep last 5 log files

  client:
    build: ../client
    container_name: chatbot_client
    ports:
      - "3000:3000"
    environment:
      - VITE_API_URL=http://localhost:8000
    volumes:
      - ../client/src:/app/src
    depends_on:
      - server

  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: chatbot_vector_db
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      ENABLE_DYNAMIC_SCHEMA: "true"

  llama3:
    image: ollama/ollama:latest
    container_name: chatbot_gemma
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    # Optional: pre-pull models on first run (commented for portability)
    entrypoint: /bin/sh
    command: -c "ollama serve & sleep 3 && ollama pull gemma:2b &&
      ollama pull nomic-embed-text && wait"
    restart: unless-stopped

volumes:
  ollama:
