# ğŸ“š Enterprise Rag Chatbot

A FastAPI-based service that ingests documents into **Weaviate**, indexes them with **LlamaIndex**, and lets you query them with semantic search.  
By default, the backend returns human-readable document chunks from Weaviate.  
Optionally, you can enable **Ollama** to summarize or generate natural answers.

---

## ğŸš€ Features

- Upload and ingest documents (`/ingest` endpoint).
- Store embeddings in **Weaviate** (vector database).
- Query stored documents with semantic similarity (`/query` endpoint).
- Pluggable LLM backend (**Ollama**) for natural language summarization.
- Lightweight REST API using **FastAPI**.
- Docker-compose ready for local development.

---

## ğŸ› ï¸ Tech Stack

- **FastAPI** â†’ REST API backend
- **Weaviate** â†’ Vector database for embeddings
- **LlamaIndex** â†’ Data ingestion & query pipeline
- **Ollama** (optional) â†’ Local LLM for summarization
- **Docker Compose** â†’ One-command environment setup

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app.py                # FastAPI backend
â”œâ”€â”€ ingest_service.py     # (Optional) Ingestion helpers
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ raw/              # Uploaded files are saved here
â”œâ”€â”€ docker-compose.yml    # Orchestration of Weaviate + Ollama + API
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # This file
```

---

## âš¡ Quickstart

### 1. Clone the repository

```bash
git clone https://github.com/gondalhafizbilal/enterprise-rag-chatbot.git
cd enterprise-rag-chatbot
```

### 2. Start services with Docker Compose

```bash
docker compose up -d
```

This will start:

- `weaviate` at `http://localhost:8080`
- `ollama` at `http://localhost:11434` (if enabled)
- `fastapi` backend at `http://localhost:8000`

### 3. Install Python dependencies (for local dev)

```bash
pip install -r requirements.txt
```

---

## ğŸ“¥ Ingest Documents

Upload a document (PDF, text, etc.):

```bash
curl -X POST "http://localhost:8000/ingest"   -F "file=@manual-test-001.pdf"
```

The file is stored in `storage/raw/` and indexed into Weaviate.

---

## ğŸ” Query the Knowledge Base

Ask a question about your documents:

```bash
curl -X POST "http://localhost:8000/query"   -H "Content-Type: application/json"   -d '{"q": "What is the refund policy?"}'
```

Response:

```json
{
  "answer": "The refund policy states that customers can request a refund within 30 days...",
  "source_nodes": ["a4f6773d-5c58-46a3-ac57-b130816f9bc1"]
}
```

---

## ğŸ§  Human-Readable Mode vs LLM Mode

- **Default mode** â†’ returns retrieved chunks directly (no LLM). âœ… Fast & reliable.
- **LLM mode** (optional) â†’ summarize into a natural answer using Ollama.

Switch mode by editing `/query`:

```python
engine = index.as_query_engine(similarity_top_k=5)  # LLM mode (can timeout if Ollama is slow)
```

---

## ğŸ§ª Health Check

Check if services are running:

```bash
curl http://localhost:8000/health
```

Example:

```json
{
  "status": "ok",
  "weaviate_ready": true,
  "ollama_host": "http://llama3:11434",
  "model": "llama3"
}
```

---

## ğŸ—‘ï¸ Reset Index

To clear everything:

```bash
docker compose down -v
```

---

## ğŸ”® Roadmap

- âœ… Ingest + query basic documents
- âœ… Support for Weaviate vector DB
- â¬œï¸ LLM summarization toggle (switch between human-readable + generated answers)
- â¬œï¸ Frontend UI (React-based chat interface)
- â¬œï¸ Authentication for production use

---

## ğŸ“ License

MIT License.  
Feel free to use, modify, and share.
